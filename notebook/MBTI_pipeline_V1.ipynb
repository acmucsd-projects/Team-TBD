{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "from torch import nn\n",
    "import csv\n",
    "import string\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "# Import utility functions\n",
    "sys.path.append(os.path.relpath(\"../python_files/\"))\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/Users/youzezheng/Desktop/Team-TBD/input/mbti_1.csv\")\n",
    "\n",
    "data = []\n",
    "for l in csv.reader(f, delimiter=','):\n",
    "    if l == ['type', 'posts']: continue\n",
    "    data.append(l)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = util.get_wordCounts(data) # slow as stemmer is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x[1] for x in counts[:2200]] # use the top 2.2K words out of 31K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode words into wordvec using BoW model with stemmer\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join(c for c in datum.lower() if not c in punctuation)\n",
    "    for w in r.split():\n",
    "        w = stemmer.stem(w)\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) # offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = [feature(post) for _,post in data] # slow as stemmer is used, plus the posts are long - I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw = [type for type,post in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP',\n",
       "       'INFJ', 'INFP', 'INTJ', 'INTP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP'],\n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode y into numerical values as tensorflow does not support str conversion\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y_raw)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw = [le.transform([y])[0] for y in y_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y_raw = np.array(X_raw), np.array(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6940, 1735, 6940, 1735)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn data into tensors\n",
    "X = torch.from_numpy(X_raw).type(torch.float)\n",
    "y = torch.from_numpy(y_raw).type(torch.LongTensor)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                     y,\n",
    "                                                     test_size=0.2,\n",
    "                                                     random_state=42)\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 9., 10., 19.,  ...,  0.,  0.,  1.],\n",
       "         [55., 30., 40.,  ...,  0.,  0.,  1.],\n",
       "         [20., 23., 18.,  ...,  0.,  0.,  1.],\n",
       "         [37., 29., 35.,  ...,  0.,  0.,  1.],\n",
       "         [23., 30., 24.,  ...,  0.,  0.,  1.]]),\n",
       " tensor([ 8,  3, 11, 10,  2]))"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2501"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBTIModel(\n",
       "  (linear_layer_stack): Sequential(\n",
       "    (0): Linear(in_features=2501, out_features=35, bias=True)\n",
       "    (1): Linear(in_features=35, out_features=35, bias=True)\n",
       "    (2): Linear(in_features=35, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(le.classes_) # number of MBTI types in our dataset\n",
    "NUM_FEATURES = len(X[0])\n",
    "class MBTIModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
    "\n",
    "        Args:\n",
    "            input_features (int): Number of input features to the model.\n",
    "            out_features (int): Number of output features of the model\n",
    "              (how many classes there are).\n",
    "            hidden_units (int): Number of hidden units between layers, default 8.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            # nn.LSTM(input_features, output_features, 5),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of MBTIModel and sent it to target device\n",
    "model = MBTIModel(input_features=NUM_FEATURES,\n",
    "                  output_features=NUM_CLASSES,\n",
    "                  hidden_units=35).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function and optimizer for a multi-class model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=0.1) # could try SGD too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0287, -0.2690, -0.4137, -0.6817, -0.1007, -0.4945, -0.3480,  0.0864,\n",
       "         -0.3105, -0.2135, -0.1478, -0.1605, -0.0917, -0.7096,  0.6299,  0.0422],\n",
       "        [ 0.0079, -0.1958, -0.5976, -0.9262,  0.2049, -0.6128, -0.1823, -0.1436,\n",
       "         -0.2891, -0.4590, -0.4136, -0.2575,  0.0351, -0.8511,  0.6886,  0.1412],\n",
       "        [-0.1356, -0.1923, -0.2864, -0.6656, -0.3008, -0.3839, -0.1643,  0.2958,\n",
       "         -0.4466, -0.1851,  0.0525, -0.1569, -0.1953, -0.7008,  0.3427,  0.0913],\n",
       "        [ 0.0020, -0.4670, -0.6625, -1.1860, -0.0717, -0.5425, -0.4724,  0.0564,\n",
       "         -0.7502, -0.4842, -0.3550,  0.1717, -0.2793, -1.0026,  0.9846,  0.0066],\n",
       "        [ 0.0264, -0.2010, -0.1864, -0.3728, -0.3569, -0.4630,  0.0425,  0.0987,\n",
       "         -0.0767, -0.3411, -0.1415, -0.2618, -0.0334, -0.4176,  0.3338,  0.1463]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a single forward pass on data\n",
    "model(X_train.to(device))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]), 16)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train.to(device))[0].shape, NUM_CLASSES # confirm shapes match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0300, -0.0225, -0.6366, -0.7175, -0.1303, -0.3949, -0.3059,  0.0781,\n",
      "         -0.3546, -0.3503, -0.1587,  0.0295, -0.1413, -0.5800,  0.4380,  0.1991],\n",
      "        [-0.3721, -0.1220, -0.6799, -1.2105,  0.3457, -0.4200, -0.0728,  0.0071,\n",
      "         -0.4094, -0.4222, -0.2629, -0.3122,  0.1463, -1.2966,  0.3303, -0.1823],\n",
      "        [ 0.1758, -0.4490, -0.2295, -0.4478, -0.2209, -0.4565, -0.2272,  0.2732,\n",
      "          0.0018, -0.4225, -0.0256,  0.2636, -0.1290, -0.3790,  0.3513, -0.0247],\n",
      "        [ 0.6173, -0.2009, -0.8345, -0.6599, -0.4933, -0.6949, -0.2842, -0.2516,\n",
      "         -0.0709, -0.7576, -0.8118,  0.1931,  0.1286, -0.1322,  0.7799,  0.0592],\n",
      "        [-0.0092, -0.4654, -0.4514, -0.8745,  0.1315, -0.7668, -0.1422,  0.0162,\n",
      "          0.0057, -0.4463, -0.3161, -0.1427,  0.0524, -0.7696,  0.6787, -0.2171]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0702, 0.0707, 0.0383, 0.0353, 0.0635, 0.0487, 0.0533, 0.0782, 0.0507,\n",
      "         0.0510, 0.0617, 0.0745, 0.0628, 0.0405, 0.1121, 0.0883],\n",
      "        [0.0536, 0.0688, 0.0394, 0.0232, 0.1099, 0.0511, 0.0723, 0.0783, 0.0516,\n",
      "         0.0510, 0.0598, 0.0569, 0.0900, 0.0213, 0.1082, 0.0648],\n",
      "        [0.0811, 0.0434, 0.0541, 0.0435, 0.0545, 0.0431, 0.0542, 0.0894, 0.0681,\n",
      "         0.0446, 0.0663, 0.0885, 0.0598, 0.0466, 0.0966, 0.0663],\n",
      "        [0.1272, 0.0561, 0.0298, 0.0355, 0.0419, 0.0342, 0.0516, 0.0533, 0.0639,\n",
      "         0.0322, 0.0305, 0.0832, 0.0780, 0.0601, 0.1497, 0.0728],\n",
      "        [0.0723, 0.0458, 0.0465, 0.0305, 0.0833, 0.0339, 0.0633, 0.0742, 0.0734,\n",
      "         0.0467, 0.0532, 0.0633, 0.0769, 0.0338, 0.1439, 0.0588]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Make prediction logits with model\n",
    "y_logits = model(X_test.to(device))\n",
    "\n",
    "# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "print(y_logits[:5])\n",
    "print(y_pred_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum the first sample output of the softmax activation function\n",
    "torch.sum(y_pred_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0702, 0.0707, 0.0383, 0.0353, 0.0635, 0.0487, 0.0533, 0.0782, 0.0507,\n",
      "        0.0510, 0.0617, 0.0745, 0.0628, 0.0405, 0.1121, 0.0883],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "# Which class does the model predict\n",
    "print(y_pred_probs[0])\n",
    "print(torch.argmax(y_pred_probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ISTJ'], dtype='<U4')"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([torch.argmax(y_pred_probs[0])]) # predict actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct/len(y_pred))*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 1420.18225, Acc: 21.11% | Test loss: 1923.36633, Test acc: 2.593660%\n",
      "Epoch: 20 | Loss: 1184.35339, Acc: 2.32% | Test loss: 787.04443, Test acc: 12.276657%\n",
      "Epoch: 30 | Loss: 586.90234, Acc: 2.09% | Test loss: 392.07782, Test acc: 4.322767%\n",
      "Epoch: 40 | Loss: 357.87659, Acc: 18.62% | Test loss: 170.29587, Test acc: 20.000000%\n",
      "Epoch: 50 | Loss: 183.95209, Acc: 20.98% | Test loss: 120.40254, Test acc: 25.706052%\n",
      "Epoch: 60 | Loss: 61.55637, Acc: 40.32% | Test loss: 75.43159, Test acc: 26.282421%\n",
      "Epoch: 70 | Loss: 23.27946, Acc: 55.55% | Test loss: 48.19202, Test acc: 29.106628%\n",
      "Epoch: 80 | Loss: 13.36421, Acc: 58.86% | Test loss: 22.38182, Test acc: 43.746398%\n",
      "Epoch: 90 | Loss: 7.13268, Acc: 68.30% | Test loss: 15.07937, Test acc: 53.429395%\n",
      "Epoch: 100 | Loss: 4.07195, Acc: 75.59% | Test loss: 12.85288, Test acc: 54.293948%\n",
      "Epoch: 110 | Loss: 2.51904, Acc: 80.06% | Test loss: 11.17986, Test acc: 54.985591%\n",
      "Epoch: 120 | Loss: 1.53566, Acc: 84.55% | Test loss: 10.41117, Test acc: 54.351585%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 120\n",
    "\n",
    "# Put data on the target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "prev_test_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_logits = model(X_train) # model outputs raw logits\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "    \n",
    "    # Calculate the loss/accuracy\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_true=y_train,\n",
    "                      y_pred=y_pred)\n",
    "    \n",
    "    # Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    \n",
    "    ### Testing\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Forward pas\n",
    "        test_logits = model(X_test)\n",
    "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "        \n",
    "        # Calculate the loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "        # if test_loss <= prev_test_loss: prev_test_loss = test_loss\n",
    "        # else:\n",
    "        #     print(f\"Stoping Epoch: {epoch+1} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:2f}%\")\n",
    "        #     break\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 9:\n",
    "        print(f\"Epoch: {epoch+1} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
