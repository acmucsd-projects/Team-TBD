<!-- <p align="center">
  <img src="https://media.tenor.com/On7kvXhzml4AAAAj/loading-gif.gif" />
</p> -->
![Waiting for Prediction](/img/Demo-2.png)

# Team TBD
The official ACM AI Team TBD repository.

## Table of Contents:
- [Requirements](https://github.com/acmucsd-projects/Team-TBD#requirements-bangbang)
- [Instructions](https://github.com/acmucsd-projects/Team-TBD#instructions-computer)
- [Datasets](https://github.com/acmucsd-projects/Team-TBD#datasets-hammer)
- [Difficulties](https://github.com/acmucsd-projects/Team-TBD#difficulties-x)
- [Reflection](https://github.com/acmucsd-projects/Team-TBD#source-bulb)
- [Author Info](https://github.com/acmucsd-projects/Team-TBD#author-info-trident)

## Requirements :bangbang:

Refer to `requirements.txt` for the packages and dependencies.

## Instructions :computer:
Clone this repository to get started.
```
git clone https://github.com/acmucsd-projects/Team-TBD.git
```

It's recommended to run our code in a virtual environment to make sure your package versions are not messed up. The following is the instruction to create a conda virtual environment for our project, where we use Python 3.9 -- a stable release:
```
conda create -n mbti python=3.9
```

To install the required packages for our project, run the following comment:
```
pip install -r requirements.txt
```

To set up the dataset for our project, click the link under [Datasets](https://github.com/acmucsd-projects/Team-TBD#datasets-hammer). Then, move the datasets into `input` folder of this repository.

## Datasets :hammer:
[(MBTI) Myers-Briggs Personality Type Dataset](https://www.kaggle.com/datasets/datasnaek/mbti-type/data)

## Difficulties :x:
Since we are students, we had a lot of classwork to do and exams coming up while we were doing this project, making scheduling times to work on it and meet together very challenging. Most of us were brand new to these new machine learning tools so we had to learn a lot as we went through it. The datasets that were provided online were very skewed for the I personality types, making the model overfitted and giving bad accuracy initially.

## Reflection :bulb:
We believe our current project could be further developed into making a more responsive, personalized chatbot. We had trouble making progress on the project due to scheduling conflicts, and having multiple meetings a week proved difficult. We believe future projects could benefit from smaller group sizes or groups based on similar interests.

Also, we could've tried to RoBERTa model, which was trained on a larger set of text corpus -- this might give us a better training result.

## Author Info :trident:
- Mentor: Vincent Tu [LinkedIn](https://www.linkedin.com/in/vincent-tu-422b18208/) | [GitHub](https://github.com/alckasoc)
- Catherine Zhang [LinkedIn](https://www.linkedin.com/in/catherine-zhang-b6a86415a/) | [GitHub](https://github.com/caz002)
- Aryaman Dayal [LinkedIn](https://www.linkedin.com/in/aryaman-dayal-a6a4b0214/) | [GitHub](https://github.com/aryamandayal)
- Hargen Zheng [LinkedIn](https://www.linkedin.com/in/hargen-zheng-75540b218/) | [GitHub](https://github.com/hgnzheng)
- Sia Patodia [LinkedIn](https://www.linkedin.com/in/sia-patodia-21a071200/) | [GitHub](https://github.com/siapatodia8)
- Phillip Wu [GitHub](https://github.com/philliptwu)
- Ryan Wong [LinkedIn](https://www.linkedin.com/in/ryanjmwong/) | [GitHub](https://github.com/RyanJWong)
